import os
os.system('pip install tiktoken')
os.system('pip install transformers_stream_generator')

import gradio as gr
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch
# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'

def clear_session():
    return '', None


from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download
from modelscope import GenerationConfig

model_dir = snapshot_download('qwen/Qwen-14B-Chat', revision='v1.0.4')

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, revision='v1.0.4')

model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True,torch_dtype=torch.bfloat16, revision='v1.0.4').eval()

model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config.top_p = 0.8
model.generation_config.repetition_penalty = 1.1


def generate_chat(input: str, history = None):
    if input is None:
        input = ''
    if history is None:
        history = []
    history = history[-5:]
    gen = model.chat_stream(tokenizer, input, history=history)
    for x in gen:
        history.append((input, x))
        yield None, history
        history.pop()
    history.append((input, x))
    return None, history

block = gr.Blocks()
with block as demo:

    # æ·»åŠ çŸ¥è¯†åº“é€‰æ‹©ä¸‹æ‹‰åˆ—è¡¨
    knowledge_base_selector = gr.Dropdown(
        choices=['å…¬å¸åˆ¶åº¦', 'å…šå‘˜å­¦ä¹ ', 'å…¶ä»–é€‰é¡¹'],
        label='é€‰æ‹©çŸ¥è¯†åº“'
    )

    gr.Markdown("""<p align="center"><img src="https://modelscope.cn/api/v1/models/qwen/Qwen-VL-Chat/repo?Revision=master&FilePath=assets/logo.jpg&View=true" style="height: 80px"/><p>""")
    gr.Markdown("""<center><font size=8>Qwen-14B-Chat BotğŸ‘¾</center>""")
    gr.Markdown("""<center><font size=4>é€šä¹‰åƒé—®-14Bï¼ˆQwen-14Bï¼‰ æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„140äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚</center>""")

    chatbot = gr.Chatbot(lines=10,label='Qwen-14B-Chat', elem_classes="control-height")
    message = gr.Textbox(lines=2, label='Input')

    with gr.Row():
        clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
        sumbit = gr.Button("ğŸš€ å‘é€")

    #sumbit.click(generate_chat,
    #           inputs=[message, chatbot],
    #           outputs=[message, chatbot])


    # ä¿®æ”¹æŒ‰é’®çš„ click æ–¹æ³•ä»¥åŒ…å«çŸ¥è¯†åº“é€‰æ‹©
    sumbit.click(generate_chat,
               inputs=[message, knowledge_base_selector, chatbot],
               outputs=[message, chatbot])




    clear_history.click(fn=clear_session,
                        inputs=[],
                        outputs=[message, chatbot],
                        queue=False)

demo.queue().launch(height=800, share=False)
